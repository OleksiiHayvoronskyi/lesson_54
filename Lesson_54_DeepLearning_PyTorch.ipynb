{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Завдання**\n",
    "### Використовуючи з лекції датасет і код:\n",
    "* створити на основі двошарової нейромережі тришарову;\n",
    "* оцінити її похибку та точність визначення класів.\n",
    "#### Після цього використати для оцінки похибки **Mean Squared Error**, в ролі оптимізатора - **Adam**.\n",
    "* Порівняти, чи змінилась похибка та точність."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Required libraries.\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "\n",
    "from tensorflow.keras.datasets import mnist # Біліотека з базою Mnist\n",
    "from tensorflow.keras.models import Sequential # Підключаємо клас моделі Sequential\n",
    "from tensorflow.keras.layers import Dense # Підключаємо клас Dense - повнозв'язний шар\n",
    "from tensorflow.keras.optimizers import Adam # Підключаємо оптимізатор Adam\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 10000\n    Root location: E:\\Python DEVELOPER\\StartUpAcademy\\projects\\lesson_54\\DATA\n    Split: Test\n    StandardTransform\nTransform: ToTensor()"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transform = transforms.ToTensor()\n",
    "\n",
    "# Downloading dataset and making a train data.\n",
    "train_data = datasets.MNIST(root='E:\\Python DEVELOPER\\StartUpAcademy\\projects\\lesson_54\\DATA',\n",
    "                            train=False, download=True, transform=Transform)\n",
    "train_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset MNIST\n    Number of datapoints: 10000\n    Root location: E:\\Python DEVELOPER\\StartUpAcademy\\projects\\lesson_54\\DATA\n    Split: Test\n    StandardTransform\nTransform: ToTensor()"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a test data.\n",
    "test_data = datasets.MNIST(root='E:\\Python DEVELOPER\\StartUpAcademy\\projects\\lesson_54\\DATA', train=False, download=False, transform=Transform)\n",
    "test_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Model's parameters.\n",
    "input_layer = 784\n",
    "hidden_layers = [128, 64]\n",
    "output_layer = 10\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "Classification(\n  (input_layer): Linear(in_features=784, out_features=128, bias=True)\n  (hid_layer1): Linear(in_features=128, out_features=64, bias=True)\n  (hid_layer2): Linear(in_features=128, out_features=64, bias=True)\n  (output_layer): Linear(in_features=64, out_features=10, bias=True)\n)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a class for neural network.\n",
    "class Classification(nn.Module):\n",
    "    # Layers initialisation.\n",
    "    def __init__(self, input_layer, hidden_layers, output_layer):\n",
    "        super().__init__() # успадковуємо всі властивості + додаємо своє.\n",
    "        self.input_layer = nn.Linear(input_layer, hidden_layers[0]) # input layer.\n",
    "        self.hid_layer1 = nn.Linear(hidden_layers[0], hidden_layers[1]) # hidden layer 1.\n",
    "        self.hid_layer2 = nn.Linear(hidden_layers[0], hidden_layers[1]) # hidden layer 2\n",
    "        self.output_layer = nn.Linear(hidden_layers[1], output_layer) # output layer.\n",
    "\n",
    "    # Path of moving data through the neural network.\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.input_layer(x))  # input layer with a relu activation.\n",
    "        out = F.relu(self.hid_layer1(out)) # hidden layer 1 with a relu activation.\n",
    "        out = F.relu(self.hid_layer2(out)) # layer 2 with relu activation.\n",
    "        out = self.output_layer(out) # output layer.\n",
    "        return F.log_softmax(out, dim=1) # activation with softmax\n",
    "\n",
    "# Making an object for the neural network.\n",
    "model = Classification(input_layer, hidden_layers, output_layer)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading data into batches:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "train_load = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_load = DataLoader(test_data, batch_size=500, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The error function and optimizer:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Initialisation the error function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Initialization the Adam optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train the Neural Network:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE [Initial data shape]: torch.Size([100, 1, 28, 28])\n",
      "AFTER [Data after reshaping]: torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_load:\n",
    "    print('BEFORE [Initial data shape]:', images.size())\n",
    "    break\n",
    "print('AFTER [Data after reshaping]:', images.view(100, -1).size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
